This guide covers the basics of online survey design, response collection, and analysis. It's not a how-to for the steps associated with any specific survey tool. Rather, it provides a general framework that will help select and use the best survey creation and survey distribution tools for your purpose.

== Steps ==

=== Understanding the Overall Steps ===

# Keep in mind the three main steps of using online surveys.
#* Survey design: This means creating the survey. It includes the selection of questions, selection of question order, selection of answer formats (e.g., free-form versus multiple-choice), and selection of design logic.<ref name=pew-questionnaire-design>[http://www.pewresearch.org/methodology/u-s-survey-research/questionnaire-design/ Questionnaire design], Pew Research Center</ref> It also includes the selection of the survey design tool.
#* Response collection: This means getting people to respond to the survey. When the total audience that you are trying to reach is small and responsive, this step is simple: you just ask them to fill in the survey. If, however, you want to get information about a large population that can't easily be reached, you'll need to find ways to properly ''sample'' from that large audience.
#* Data analysis: This involves looking at the survey responses and figuring out the answers to the questions that motivated you to make the survey in the first place.
# Keep in mind the dependencies between the selection of tools for these purposes.
#* In ''principle'', the steps are independent: you could use one tool for survey design, another tool for response collection, and a third tool for data analysis. In practice, there are limitations that they place on each other.
#* Some survey collection tools ''require'' the survey to be hosted on the associated survey design platform. For instance, if using Google Consumer Surveys for response collection, you have to host the survey on Google Consumer Surveys. Similarly, if using SurveyMonkey Audience for response collection, you have to host the survey on SurveyMonkey. On the other hand, you can use Amazon Mechanical Turk to get responses to surveys hosted elsewhere: you just have to tell the Mechanical Turk participants to go to the survey link.
#* Conversely, some survey design tools only allow for response collection through the associated response collection tool. For instance, if you design a survey in Google Consumer Surveys, you can only direct responses to it through Google Consumer Surveys. On the other hand, surveys designed using SurveyMonkey can be distributed both through SurveyMonkey Audience and to your own selected audiences through the sharing of web links (with multiple collectors possible on a single survey).
#* Survey analysis can usually be done on the survey design platform itself. Many survey design platforms offer the option of exporting the survey results in formats such as the computer-friendly CSV format. You can process these exports through your own data analysis tools. However, the latter might require the expertise of a data scientist. Thus, much of your analysis of survey results is likely to be on the survey design platform itself.
# Keep in mind the interaction with your budget and goals. To use the right survey solution in a cost-effective way, you need to juggle a lot of different things. This guide will prepare you!
#* You need to have a good idea of the goals you wish to accomplish.
#* You need to have reasonable familiarity with survey design (something you will hopefully have after reading this guide). In cases where your target audience is too huge to survey completely, you'll also need the right statistical intuition and tools to interpret the results.
#* You need to know what your budget is. This becomes important when you are trying to reach reasonably representative samples of large target audiences.
#* You need a good mental model of the dependencies between the different components.

=== Selecting a Survey Design Tool ===

# Keep in mind the different functionalities offered by a survey design tool.
#* A survey design tool makes it easier for you to construct survey questions. Rather than write code to describe all the logic, you can select from pre-existing templates and designs. 
#* Some survey design tools also provide intelligent guidance on how to make your survey questions more statistically reliable.
#* The survey design tool also generally ''hosts'' the survey, and stores both the actual survey and all response information to the survey.
#* The survey design tool might have associated response collection tools (for instance, survey design tool SurveyMonkey has associated response collection tool SurveyMonkey Audience). Alternatively, it might simply allow you to generate a weblink that you can circulate to the desired survey audience. Some tools offer both.
#* The survey design tool provides some rudimentary online analysis of the survey results, and also allows you to download and export the responses. Typical download formats include comma-separated values (CSV), Microsoft Excel (XLS or XLSX), and Portable Document Format (PDF).
#* It is possible to imagine in principle that the actual question design part and the hosting part are completely separate: you can design a survey in one tool and then host it and collect responses on a different tool. However, in practice, most of the leading survey design tools combine both functions. Part of the reason is that online surveys involve reasonably complex web logic for which there is no global standard specification, and each tool differs somewhat in the capabilities it supports. So, you cannot upload a survey from one tool to another.
# Keep in mind the following limitations placed by your survey design tool.
#* Some survey design tools, such as Google Forms, ''only'' allow web links, and don't directly integrate with response collection.<ref name=google-forms-response-collection>[https://support.google.com/docs/answer/2917686 Choose a form response destination], Google Support, September 24, 2016</reF> That means that response collection is ''your'' job: you'll need to take the web link and circulate it through your own distribution channels.
#* Survey design tools may not support features you care about: question types you want to use, branching and skip logic, ability to edit questions after the survey has started, ability to maintain multiple collectors for a single survey, integration with data analysis capabilities, unlimited responses, and data export capabilities. Some design tools (such as SurveyMonkey) offer some of these features in the free version, but restrict other, more advanced features to the paid version.<ref name=surveymonkey-pricing>[https://www.surveymonkey.com/pricing/upgrade/details/ SurveyMonkey Pricing]</ref>
# Use existing online comparisons of survey design tools. The Wikipedia page comparing survey software provides a starting point of a list.<ref name=wikipedia-comparison>[https://en.wikipedia.org/wiki/Comparison_of_survey_software Comparison of survey software], Wikipedia, the free encyclopedia, retrieved September 24, 2016</ref> WordStream also has a review of the best online survey tools, albeit this is from 2014 so some information may be outdated.<ref name=wordstream-survey-tool-comparison>[http://www.wordstream.com/blog/ws/2014/11/10/best-online-survey-tools 7 Best Survey Tools: Create Awesome Surveys For Free!], WordStream, November 10, 2014</ref> Some of the most common survey design tools, and their key advantages and limitations, are discussed below.
#* Google Forms
#** It is free and offers unlimited responses
#** it integrates with Google Sheets, where you can use spreadsheet commands to understand the responses. Responses can also be exported to Excel, CSV, or PDF for response analysis.<ref name=google-forms-response-collection/>
#** You can create reasonably complex surveys and question types.<ref name=google-forms-create-survey>[https://support.google.com/docs/answer/87809?hl=en&ref_topic=6063584 Create a survey using Google Forms], Google Support, retrieved September 24, 2016</ref> 
#** Upshot: It can be an ideal situation for a reasonably simple survey, and may be the best free tool available if you're getting over 1000 responses. 
#** One key ''disadvantage'' is a lack of integration with any response collection methods: you're basically left to your own devices for getting respondents.<ref name=google-forms-response-collection/>
#* SurveyMonkey 
#** Many pricing tiers starting with free (the BASIC plan).<ref name=surveymonkey-pricing/> 
#** BASIC plan good enough for most use cases: If your surveys have a linear flow (no skip logic or branching) and you don't expect over 100 responses per survey, the free version is good enough for you. 
#** Integration with SurveyMonkey Audience for response collection is a helpful feature (although SurveyMonkey Audience costs money every time you use it, it can be used with the free version of SurveyMonkey). 
#** You need to pay for advanced features such as: skip logic, over 100 responses on any survey, full data exports (beyond just PDF of a summary), multiple filters, and more. The cheapest plan is $26/month or $300/year. It is usually easy to switch down from paid to free, and you can continue to access past survey responses with over 100 responses.<ref name=surveymonkey-downgrading>[http://help.surveymonkey.com/articles/en_US/kb/Downgrading Downgrading to a BASIC Plan], SurveyMonkey, retrieved September 24, 2016</ref> 
#** One key ''disadvantage'' is that data export isn't available in the free version, though you can still make publicly shareable online links. But it means you are tied to the online platform unless you pay for at least one month, or manually copy over the data.<ref name=wordstream-survey-tool-comparison/> The other key disadvantage is the inability to view over 100 responses per survey, which can be an issue since you may need more than that many responses to get your entire target audience, or a large enough sample.<ref name=surveymonkey-sample-estimation>[http://help.surveymonkey.com/articles/en_US/SurveyMonkeyArticleType/How-many-respondents-do-I-need Calculating the Number of Respondents You Need], SurveyMonkey, retrieved September 24, 2016</ref><ref name=surveymonkey-significant-differences>[http://help.surveymonkey.com/articles/en_US/kb/Significant-Differences Statistical Significance], SurveyMonkey, retrieved September 24, 2016</ref>
#* Google Consumer Surveys
#** Google Consumer Surveys as a survey design tool is tied to Google Consumer Surveys as a response collection tool: to use one you have to use the other. Therefore, you should use this for design only if you are okay with using it to get responses. In particular, don't use this to survey you own friends or people in a small audience of yours.
#** It makes sense for short surveys, and is most ideal for single-question surveys.
#* Survata is a survey design tool with a similar model as Google Consumer Surveys: it is tied to its own distribution mechanism for the surveys. The advantages and disadvantages of Survata are discussed more in the section of choosing a response collection tool.<ref name=survata-v-gcs-dunrie>[http://scientificink.com/online-consumer-survey-comparison/ Online Consumer Survey Comparison: Survata vs. Google Consumer Surveys], Dunrie, Scientific Link, September 10, 2014</ref><ref name=survata-v-gcs-mmr>[http://mmrstrategy.com/comparing-online-consumer-surveys-survata-vs-google-consumer-surveys Comparing Online Consumer Surveys: Survata vs Google Consumer Surveys], Dominique Romanowski, MMR Strategy Group, August 29, 2012</ref>
#* Facebook Polls is a survey design tool for creating surveys on Facebook. These can be circulated through Facebook the way Facebook posts usually circulate (with the circulation limited by the privacy settings of the poll and/or the group or page it was posted in). In addition, you can link to the poll from elsewhere to invite others to participate in it.<ref>[https://apps.facebook.com/my-polls Facebook Polls], retrieved September 24, 2016</ref>
#* Other survey solutions worth looking into include Qualtrics and SurveyGizmo.<ref>[http://usatoday30.usatoday.com/MONEY/usaedition/2012-08-28-Efficient-Small-Business-Ecommerce_CV_U.htm Customer research easier in digital era], Scott Martin, ''USA Today'', August 28, 2012</ref><ref name=wikipedia-comparison/>

=== Designing the Survey ===

# Keep in mind the following broad ideas.
#* There are two key concepts when evaluating how robust survey responses are: ''reliability'' and ''validity''. Reliable means that if the survey was administered again, you'd get similar responses. In other words, reliable means the survey is measuring something. Valid means that what the survey measures is what you care about. For instance, if you ask people how good they are at knitting, the survey is reliable if you get the same distribution of responses whenever you administer the survey. It's valid if their answers are actually correct, i.e., how good they say they are at knitting matches how good they are at knitting.<ref name=survey-reliability-validity>[http://www.relevantinsights.com/validity-and-reliability/ Validity and Reliability in Surveys], Michaela Mora, Relevant Insights, February 21, 2011</ref>
#* People aren't perfectly rational and they aren't omniscient or omnipotent. Human limitations can affect the reliability and validity of surveys. However, not all is lost: by understanding the ways in which humans tend to mess up, you can improve your survey design. The systematic ways humans deviate from rationality are called cognitive biases.<ref name=cognitive-bias-codex>[https://betterhumans.coach.me/cognitive-bias-cheat-sheet-55a472476b18 Cognitive bias cheat sheet. Because thinking is hard.], Buster Benson, Better Humans, September 1, 2016</ref><ref>[http://lifehacker.com/this-graphic-explains-20-cognitive-biases-that-affect-y-1730901381 This Graphic Explains 20 Cognitive Biases That Affect Your Decision-Making], Patrick Allan, LifeHacker, September 16, 2015</ref>
#* Cognitive biases come in four clusters that arise from the following four phenomena: too much information, not enough meaning, need to act fast, and difficulty of remembering everything over the long term.<ref name=cognitive-bias-codex/> All of these influence how people respond in surveys. In the context of surveys, the need to act fast is the biggest factor, and too much information and not enough meaning can also play a role depending on the types of questions at hand.
#* The following are the specific biases most relevant to survey design:
#** The observer-expectancy effect, where survey respondents tailor their responses based on what they think the question-asker wants to hear. This is one of the reasons that question order matters.
#** Social desirability bias, where respondents try to answer in ways that make them look good.
#** Availability bias and related issues (such as the anchoring heuristic, bizarreness effect, base rate fallacy) where recently presented or other related information distorts people's answers.
# Keep in mind the following key points around things to watch out for as you design surveys.
#* The order of things matters. Randomization (at various levels) and A/B testing between different orders can help address this issue and determine the role of order.
#* The framing of questions matters. Some framings help combat biases (by cautioning people, or by addressing concerns such as those related to social desirability).
# Make sure each question makes sense to all your audience.
#* Think about whether your framing is inclusive to most of your target audience. In particular, make sure your questions make sense for respondents for all values of age groups, genders, occupation status, location, language, and any other criteria you care about. 
#** An egregious example of forgetting this is, for instance, assuming that your survey respondent is male, but then circulating it to a mixed gender audience.
#** Subtler errors can arise in case that your implicit assumption holds for the majority of respondents you have in mind. For instance, a survey you target at United States college students might implicitly assume that the students live on campus or close to it, based on your own college experience. But many of your potential respondents might be commuter students.
#* In cases where your entire survey is aimed at a segment of the audience to which it will inevitably be distributed, use disqualifying questions right at the beginning. This saves time for people who don't meet your criteria, as they can get disqualified early on.
#* In cases where some questions make sense only for a subset of the audience, but the survey as a whole is of interest to the entire audience, offer options of a "Not applicable" nature for the rest of your audience.
#* For more complex surveys, where subsets of the survey make sense only for subsets of the audience, use branch and skip logic.
# Frame your questions to tackle social desirability bias. Social desirability bias means that people say things that cast them in a good light. This is partly because they are fooling themselves.
#* Social desirability bias is both a characteristic of the individual survey respondent (some respondents are more prone to this bias than the others) and of the survey context.<ref name=wikipedia-sdb>[https://en.wikipedia.org/wiki/Social_desirability_bias Social desirability bias], Wikipedia, the free encyclopedia</ref> There is a Marlow-Crowne Social Desirability Bias Scale to judge the extent to which a given respondent has social desirability bias, and you can administer this as part of your survey.<ref name=ace-sdb>[http://www.animalcharityevaluators.org/research/foundational-research/survey-guidelines/social-desirability-scale/ Social Desirability Scale], Animal Charity Evaluators</ref><ref name=eaf-sdb>[http://effective-altruism.com/ea/105/using_amazons_mechanical_turk_for_animal_advocacy/ Using Amazon's Mechanical Turk for Animal Advocacy Studies: Opportunities and Challenges], Peter Hurford, Effective Altruism Form, August 2, 2016</ref> You can also reduce and control for social desirability bias through the framing of your survey questions.
#* Vegetarianism is one area where social desirability bias is strong: many people who claim to be vegetarian in surveys also report having eaten meat recently.<ref name=vegetarianism-pt>[https://www.psychologytoday.com/blog/animals-and-us/201109/why-are-there-so-few-vegetarians Why Are There So Few Vegetarians? Most "vegetarians" eat meat. Huh?], Hal Herzog, ''Psychology Today'', September 6, 2011</ref><ref name=vegetarianism-sdb-econlog>[http://econlog.econlib.org/archives/2013/07/vegetarianism_a.html Vegetarianism and Social Desirability Bias], Bryan Caplan, EconLog, July 16, 2013</ref> Another example is people reporting whether they vote.<ref>[http://web.stanford.edu/dept/communication/faculty/krosnick/Turnout%20Overreporting%20-%20ICT%20Only%20-%20Final.pdf Social Desirability Bias in Voter Turnout Reports: Tests Using the Item Count Technique], Allyson L. Holbrook and Jon A. Krosnick</ref> As the vegetarianism and voting examples indicate, social desirability bias can be exhibited even for cases where the behavior in question is not universally regarded as ideal. What matters is that there are enough people who view the behavior as desirable but may still end up not doing it. The Pew Research Center identifies the following other areas where social desirability bias is strong: "Research has shown that respondents understate alcohol and drug use, tax evasion and racial bias; they also may overstate church attendance, charitable contributions and the likelihood that they will vote in an election."<ref name=pew-questionnaire-design/> Wikipedia has a similar list.<ref name=wikipedia-sdb/>
#* Anonymity is a first step to combating social desirability bias. If your online survey asks for no personally identifiable information, that is usually enough to make respondents feel anonymous. However, there are cases where this is not sufficient to guarantee anonymity, usually when the audience is quite small so that it's possible to infer people's identity from their other responses. Some techniques, such as the randomized response technique and the unmatched count technique, have evolved to address this concern.<ref>[https://en.wikipedia.org/wiki/Randomized_response Randomized response], Wikipedia, the free encyclopedia</ref><ref>[https://en.wikipedia.org/wiki/Unmatched_count Unmatched count], Wikipedia, the free encyclopedia</ref> However, even a guarantee of anonymity may not be enough, because people might also be misleading themselves!
#* One way of combating social desirability bias is to make the question very specific. For instance, instead of asking whether a person is vegetarian, ask the person whether he or she ate meat in the last day. Instead of just saying "meat" specify a list of animal products. For instance, some self-identified vegetarians eat seafood (such as fish) since they consider that within the scope of vegetarianism. By making your question explicit on whether you include fish, you can help address any confusion.<ref name=vegetarianism-pt/><ref name=vegetarianism-sdb-econlog/> For more specifically on understanding people's dietary habits, look up more information on "food frequency questionnaire".<ref name=ace-sdb/>
#* A further step is to give people ready-made reasons so that they feel less bad about answering a question in a way that looks bad to them. For instance, the Pew Research Center's guideline on questionnaire design provides the following example of a way of gently asking people about whether they voted: “In the 2012 presidential election between Barack Obama and Mitt Romney, did things come up that kept you from voting, or did you happen to vote?”<ref name=pew-questionnaire-design/>
# For any individual question, choose carefully whether you'd like to use a closed-ended format (such as multiple-choice) or open-ended format.
#* The Pew Research Center's guide to questionnaire design notes that providing explicit choices makes it likely that people will select among those choices, even if a free response option is provided. This can be both an advantage and a disadvantage, depending on whether your goal is to obtain people's free thinking or get the items from a pre-specified list of options that come closest to their thinking.<ref name=pew-questionnaire-design/> This is related to anchoring and experimenter-observancy effects (is it?).
#* If using the closed-ended format, a useful tip for generating the options is to run a pilot test or focus group. Basically, ask a few people for free responses, then discuss it with them or let them discuss with each other. Then use the results of that discussion to formulate the options.<ref name=pew-questionnaire-design/> A similar approach has been used in the design of concept inventories, tests administered to help identify student misconceptions.(add citation)
#* One way of getting the benefits of a closed-ended format and an open-ended format is to first ask people for an open-ended response, and then present a list of options and ask which one best approximates their response. This can work best if they are not allowed to go back and edit their open-ended response.(HT: Issa Rice, EOTS)
#* If offering closed-ended options, consider randomizing the sort order. Randomization is a feature offered by most survey design tools. However, in cases that the options form a natural progression (for instance, from "use a lot" or "do not use at all"), keeping the sort order fixed helps. In such cases, randomizing between the original sort order and the ''reverse'' sort order, and then comparing the performance across the two sort orders, can help.<ref name=pew-questionnaire-design/> (Is this option offered by survey design tools?).
# Keep in mind that question order can affect your responses.
#* This is related to anchoring and the experimenter-observancy effect. The survey-taker uses previous questions to anchor his or her thinking, and is (consciously or subsconsciously) figuring out where you are trying to go. Another effect to keep in mind is the desire to appear consistent.
#* The Pew Research Center has found, in its questionnaire design, that when a general question follows a specific question, people tend to exclude the specific question from consideration in answering the general question. They call this a ''contrast effect'', and also note an effect of the general question on the specific question.
#* It may be possible to randomize the order of questions, or A/B test between different question orders.
#* If later questions make people reconsider earlier questions, it may be better to re-ask earlier questions ("in light of the new information, do you think ...") while ''not allowing'' people to go back and change the answers to earlier questions. This will give you a clearer sense of how people's thoughts on each question were affected by other questions.

== References ==

<references/>