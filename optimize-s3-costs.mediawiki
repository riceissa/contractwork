Amazon's Web Services (AWS), and in particular the Simple Storage Service (S3)<ref>[http://en.wikipedia.org/wiki/Amazon_S3 Amazon S3 (Wikipedia)]</ref> are widely used by many individuals and companies to manage their data, websites, and backends. These range from isolated individuals and small startups to multi-billion-dollar valuation companies such as Pinterest<ref>[http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html Pinterest Architecture Update - 18 Million Visitors, 10x Growth,12 Employees, 410 TB of Data]</ref> and (formerly) Dropbox.<ref>[https://www.wired.com/2016/03/epic-story-dropboxs-exodus-amazon-cloud-empire/ The Epic Story of Dropboxâ€™s Exodus From the Amazon Cloud Empire]</ref> This page is not intended as a guide to onboarding S3; you can find many other such guides online.<ref>[http://www.hongkiat.com/blog/amazon-s3-the-beginners-guide/ Amazon S3: The Beginner's Guide]</ref> Rather, it is targeted at individuals and companies whose expected S3 costs are reasonably high (in excess of $100 per month) but are not big enough to spend full-time effort on cost optimization or on building their own alternative. Other similar lists of tips are available online.<ref name=sumologic>[https://www.sumologic.com/aws/s3/s3-cost-optimization/ Optimizing Costs for S3], Jacek Migdal, SumoLogic</ref>

This page also does not go into other kinds of S3 optimizations, such as optimizing bucket, folder and file names, and operation order, focused on maximizing throughput or minimizing latency. Most of these optimizations don't affect costs directly (either positively or negatively). Moreover, they generally become relevant only at a substantially greater scale than the scale that the target audience for this page is likely to be at. You can read more about such optimizations in the official S3 guide<ref>[http://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html Request Rate and Performance Considerations]</ref> and elsewhere.<ref>[https://cloudnative.io/blog/2015/01/aws-s3-performance-tuning/ AWS S3 Performance Tuning]</ref>
[[Category:Amazon]]
== Steps ==
=== Getting a broad understanding of S3 ===
#Understand your S3 use case. S3 can be used for many goals.
#* As a place to store files for live serving on websites, particularly image files.<ref>[http://www.hongkiat.com/blog/amazon-s3-the-beginners-guide/#Amazon_S3_as_Image_Hosting Amazon S3 as Image Hosting]</ref>
#* As a place for data that you consume from or generate in your applications: By logging all data in S3, you make your data independent of the EC2 instances. This allows for a more flexible EC2 instance architecture. For instance, you can temporarily spin up EC2 spot instances to read data from S3, transform it, write the transformed data back to S3, and then terminate themselves.
#* As a place to store file dumps of structured data stores such as SQL databases, as well as EBS snapshots.
#* As a place to store executables, scripts, and configurations necessary to launch new instances with your applications (or update your applications on existing instances).
#Understand the main way S3 affects costs. Numbers below are for standard storage, caveats associated with other forms of storage are discussed later.<ref name=pricing/> All costs are applied and reported separately for each bucket and each hour; in other words, if you download your detailed billing report, you will see one line item each for every combination of bucket, hour, and type of cost.
#* '''Storage costs''': The cost is measured in storage space multiplied by time. You do not pay upfront for an allocated amount of storage space. Rather, every time you use more storage, you pay extra for that extra storage for the amount of time you use it. Costs can therefore fluctuate over time as the amount of data you've stored changed. Storage costs are reported separately for each bucket every hour. The pricing varies by region but is fixed within each region. As of December 2016, the cost ranges from 2.3 cents per GB-month in US Standard (North Virginia) to 4.05 cents per GB-month in Sao Paulo.<ref name=pricing/>
#* '''Request pricing''': The cost for PUT, COPY, POST, or LIST Requests ranges from $0.005 per 1000 requests in US regions to $0.007 per 1000 requests in Sao Paulo.<ref name=pricing/> The cost for GET and all other requests is an order of magnitude smaller, ranging from $0.004 per 10000 requests in all US regions to $0.0056 per 10000 requests. Note, however, that most of the cost associated with a GET request is captured in the data transfer costs (if the request is made from outside the region).
#* '''Data transfer costs''': Costs are zero within the same AWS region (both S3 -> S3 and S3 -> EC2 instances), about 2 cents per GB for data transfer across regions, and about 9 cents per GB for data transfer to outside AWS.
# Understand the central role played by buckets in organizing your S3 files, and the use of "object" for a S3 files.
#*You can create buckets under your account. A bucket is identified by a string (the bucket name). There can be only one bucket with a given bucket name across all of S3, across customers; therefore you may not be able to use a bucket name if somebody else already uses it.
#*Each bucket is associated with an AWS region, and is replicated across multiple availability zones within that region. The availability zone information is not available to the end user, but is an internal implementation detail.
#*Within each bucket, you can store your files either directly under the bucket or in folders. Folders don't need to be created or deleted. If you save a file it will automatically bring into existence "folders" for the file path to make sense, if they don't already exist. Once there are no files underneath it, the folder automatically ceases to exist.
#*The way S3 stores the information is as a key-value store: for each prefix that is not a file name, it stores the set of files and folders with that prefix. For each file name, it maps that to the actual file. In particular, different files in a bucket may be stored in very different parts of the data center.
#*S3 calls its files "objects", and you might encounter this term when reading up about S3 elsewhere.
#Understand the pros and cons of dealing with S3 files.
#* You can use the AWS Command Line Interface,<ref>[https://aws.amazon.com/cli/ AWS Command Line Interface]</ref> the more antiquated s3cmd, or other methods to read and write files in S3. The CLI and s3cmd both use Python. If you're using Java, you can make use of the AWS Java SDK.
#* Despite appearing superficially like a filesystem, S3 is not really one. It is a key-value store. It takes a little effort to get used to the precise differences, but one of the implications is that it requires a bit more gymnastics (and running time) to get a global picture of the amount of data used in a bucket or in subfolders of that bucket. Another implication is that it is not possible to perform operations like appending data to a file: all operations are atomic.
#* From the cost perspective, one important implication of S3 not being a filesystem is that finding all the files that match a regex can be a very expensive operation, particularly when that regex includes wildcards ''in the middle of the expression'' rather than at the end.
#* S3 can support file sizes up to 5 TB, but cross-region data transfer might start getting messed up for file sizes of more than a few hundred megabytes. The CLI uses multi-part upload for large files. Make sure that if your programs deal with large files, they operate through the multi-part upload, or split the output into smaller files.
#* S3 does not provide full support for rsync. However, there is a sync command (aws s3 sync in the AWS CLI, and s3cmd sync in s3cmd) that syncs all contents between a local folder and a S3 folder, or between two S3 folders. For files that exist in both the source and the destination folder, it only sends incremental changes, similar to the rsync protocol. The main difference with rsync is that it applies to an entire folder, and file names cannot be changed.

=== Doing the obvious: zipping/compressing data where feasible ===
#Before you begin, make sure that you are compressing data where permitted by the requirements of your application.
#* Explore what forms of zipping and compression are compatible with the processes you use to generate data, and the processes you use to read and process data.
#* Make sure you are using zipping and compression for your biggest dumps of data insofar as it does not interfere with your application. In particular, raw user logs and structured data based on user activity are prime candidates for compression.
#* As a general rule, compression will save not only on storage costs but also on transfer costs (when reading/writing the data) and might even end up making your application faster if upload/download time is a bigger bottleneck than local compression/decompression time. This is often the case.
#* To take an example converting large structured data files to the BZ2 format can cause the storage space to go down by a factor varying from 3 to 10. Another commonly used compression algorithm is gzip.<ref name=sumologic/>
#If compressing data is not possible at the point where you are first writing it out, consider running an alternative process to re-ingest and compress the data. This is generally a suboptimal solution and very rarely necessary, but there may be cases where it is relevant.

=== Optimizing storage costs through storage class selection, object versioning, and lifecycle policies ===
#Understand the differences between the four types of S3 storage.<ref name=pricing/>[[Image:Optimize Your Amazon S3 Costs Step 6.jpg|center]]
#* Standard storage is the most expensive for storage, but is cheapest and fastest for making changes to data. It is designed for 99.999999999% durability (over a year, i.e., this is the expected fraction of S3 objects that will survive over a year) and 99.99% availability (availability referring to the probability that a given S3 object is accessible at a given time).  Note that in practice, it is very rare to lose data in S3, and there are bigger risk factors to data loss than data actually disappearing from S3 (these bigger factors include accidental data deletion and somebody maliciously hacking into your account to delete content, or even Amazon being forced to delete your data because of pressure from governments).<ref>[https://www.quora.com/Has-Amazon-S3-ever-lost-data-permanently Has Amazon S3 ever lost data permanently?]</ref>
#* Reduced Redundancy Storage (RRS) used to be 20% cheaper than standard storage and offers a little less redundancy. You might wish to use it for a lot of your data that is not highly critical (such as full user logs). This is designed for 99.99% durability and 99.99% availability. However, currently listed prices on the AWS website for RRS are higher than those for S3 standard, suggesting that RRS is being phased out.<ref>[https://aws.amazon.com/s3/reduced-redundancy/ Amazon S3 Reduced Redundancy Storage]</ref>
#* Standard storage - Infrequent Access (called S3 - IA) is an option introduced by Amazon in September 2015, that combines the high durability of S3 with a low availability of only 99%. It is an option for storing long-term archives that do not need to be accessed often but that, when they need to be accessed, need to be accessed quickly.<ref>[http://searchaws.techtarget.com/news/4500253771/AWS-S3-pricing-downshifts-with-new-Infrequent-Access-tier AWS S3 pricing downshifts with new Infrequent Access tier]</ref> S3 - IA is charged for a minimum of 30 days (even if objects are deleted before that) and a minimum object size of 128 KB. It is approximately half as expensive as S3, though the precise discount varies by region.
#* Glacier is the cheapest form of storage. However, Glacier costs money to unarchive and make available again for reading and writing. Also, Glacier files have a minimum 90-day storage period: files deleted before then are charged for the remainder of the 90 days upon deletion.
# Get a sense of how your costs are growing.
#* In a use case where you have a fixed set of files that you periodically update (effectively removing older versions) your monthly storage costs are approximately constant, with a fairly tight upper bound. Your cumulative storage spend grows linearly. This is a typical scenario for a set of executables and scripts.
#* In a use case where you are continually generating new data at a constant rate, your monthly storage costs grow linearly. Your cumulative storage cost grows quadratically.
#* In a use case where the rate of data generation itself is growing linearly, your monthly storage costs grow quadratically, and your cumulative storage cost grows cubically.
#* In a use case where the rate of data generation is growing exponentially, both your monthly data storage cost and your cumulative data storage cost grow exponentially as well.
# Explore whether object versioning makes sense for your goals.<ref>[http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html Using Versioning]</ref>[[Image:Optimize Your Amazon S3 Costs Step 7.jpg|center]]
#* Object versioning allows you to keep older versions of a file. One advantage is that you can revisit an older version.
#* When using object versioning, you can combine it with lifecycle policies to retire versions older than a certain age (if not the current version).
#* If using object versioning, keep in mind that just listing files (using aws s3 ls or the online interface) will cause you to underestimate the total storage used, because you are charged for older versions that aren't included in the list.
#Explore lifecycle policies for your data.[[Image:Optimize Your Amazon S3 Costs Step 8.jpg|center]]
#* You can set policies to automatically delete data in particular buckets, or even with particular prefixes within buckets, that is more than a certain number of days old. This can help you better control your S3 costs and also help you comply with various privacy and data policies. Note that some data retention laws and policies might ''require'' you to maintain data for a minimum time; these put a lower bound on the time after which you can delete data in your lifecycle policy. Other policies or laws might require you to delete data within a certain time period; these put an upper bound on the time after which need to delete data in your lifecycle policy.
#* With a lifecycle policy for deletion, the way your costs grow changes a lot. Now, with a constant stream of incoming data, your monthly storage costs remain constant rather than grow linearly, since you are storing only a moving window of data rather than all data so far. Even if the size of incoming data is growing linearly, your monthly storage costs only grow linearly rather than quadratically. This can help you tie your infrastructure costs to your revenue model: if your monthly revenue is roughly proportional to the rate at which you receive data, your storage model is scalable.
#* A technical limitation: you cannot set two policies with the same bucket where one prefix is a subset of the other. Keep this in mind as you think about how to store your S3 data.
#* In addition to lifecycle policies for deletion, you can also set policies to archive the data (i.e., convert it from standard storage to Glacier), reducing the storage costs. However, Glacier has a minimum retention period of 90 days: you are charged for 90 days of storage in Glacier, even if you choose to delete it before then. Therefore, if you intend to delete shortly, it's probably not a good idea to move to Glacier.
#* You can also have a lifecycle policy to convert data in S3 (standard storage) to S3 - IA. This policy is ideal for data that you expect to be accessed frequently in the immediate aftermath of its creation but infrequently afterward. Files in IA have a minimum object size (you are charged for 128 KB file size for smaller files than that) and a minimum 30-day retention period.

=== Optimizing data transfer costs through region selection and consider cross-region replication ===
#Understand the key co-location advantage of EC2/S3. If your primary use for S3 is to read and write data to EC2 instances (i.e., any of the use cases other than the live serving instance), then this advantage is best reaped if your S3 bucket is located in the same AWS region as the EC2 instances that read or write to it. This will have several advantages:
#* Low latency (less than a second)
#* High bandwidth (in excess of 100 Mbit/second): Note that bandwidth is actually quite good between the different US regions, so this is not a significant issue if all your regions are in the US, but it can be significant between the US and EU, EU and Asia-Pacific, or the US and Asia-Pacific.
#* No data transfer costs (however, you still pay the request pricing)<ref name=pricing>[http://aws.amazon.com/s3/pricing/ S3 Pricing]</ref>
#Determine the location (AWS region) of your S3 bucket(s).[[Image:Optimize Your Amazon S3 Costs Step 10.jpg|center]]
#* If you're running EC2 instances that read from or write to the S3 buckets: As noted in Step 1, colocation of S3 and EC2, to the extent feasible, helps with bandwidth, latency, and data transfer costs. Therefore, an important consideration in locating your S3 bucket is: where do you expect to have the EC2 instances that will interact with this S3 bucket? If the EC2 instances are mostly backend instances, then you should consider the costs of these instances. If they are frontend instances, consider what regions you expect to get most of your traffic from. By and large, you should expect EC2 instance considerations to be more important than S3 considerations in determining the region. So it generally makes sense to first decide where you expect your EC2 instance capacity to be, and then have your S3 buckets there. In general, S3 costs are lower in the same regions that EC2 instances are, so this luckily does not create a conflict.
#* If there are other AWS services that you must have, but that are not available in all regions, this might also constrain your choice of region.
#* If you are frequently uploading files from your home computer to S3, you might consider getting a bucket in a region closer to your home, to improve the upload latency. However, this should be a minor consideration relative to the others.
#* If you expect to use S3 for live-serving static images, decide the location based on where you expect to get your traffic from.
#* In some cases, the policies you are obligated to follow based on law or contract constrain your choice of region for S3 data storage. Also keep in mind that the physical location of your S3 bucket could affect what governments are able to legally compel Amazon to release your data (although such occurrences are fairly rare).<ref>[https://d0.awsstatic.com/whitepapers/compliance/AWS_EU_Data_Protection_Whitepaper.pdf Amazon Web Services: Whitepaper on EU Data Protection], November 2016</ref>
#Investigate whether cross-region replication makes sense for your bucket.<ref>[http://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html Cross-Region Replication]</ref> Cross-region replication between buckets in different regions automatically syncs up updates to data in one bucket with data in other buckets. The change may not happen immediately, and large file changes in particular are constrained by bandwidth limitations between regions. Keep in mind the following pros and cons of cross-region replication.<ref name=sumologic/>[[Image:Optimize Your Amazon S3 Costs Step 11.jpg|center]]
#* You pay more in S3 storage costs, because the same data is mirrored across multiple regions.
#* You pay in S3 <-> S3 data transfer costs. However, if the data is being read or written by EC2 instances in multiple regions, this might be offset by savings in the S3 -> EC2 data transfer costs. The main way this can help is if you are loading the same S3 data into EC2 instances in many different regions. For instance, suppose you have 100 instances each in US East and US West where you need to load the same data from a S3 bucket in US West. If you do not replicate this bucket in US East, you pay for the transfer cost of the 100 data transfers from the S3 bucket to the US East machines. If you replicate the bucket in US East, you pay only once for the data transfer costs.
#* Cross-region replication thus makes a lot of sense for executables, scripts, and relatively static data, where you value cross-region redundancy, where updates to the data are infrequent, and where most of the data transfer is in the S3 -> EC2 direction. Another advantage is that if this data is replicated across regions, it's much faster to spin up new instances, enabling more flexible EC2 instance architectures.
#* For logging applications (where data is being read by many frontend instances and needs to be logged in a central location in S3) it is better to use a service such as Kinesis to collate data streams across regions rather than use cross-region replicated S3 buckets.
#* If you are using S3 for live-serving of static images on a website, cross-region replication may make sense if your website traffic is global and rapid loading of images is important.
# If syncing regular updates to already existing files, choose a folder structure that allows the use of the AWS CLI's sync feature.
#* The "aws s3 sync" command behaves like rsync, but can only be run at the level of a folder. Therefore, keep your folder structure such that you can use this command.

=== Optimizing cost due to request pricing ===
#Since request pricing is usually very cheap compared to the other pricing components, implement the suggestions here ''only if'' you are among the rare S3 users who make a large number of requests.
#If you are subdividing data across files, use a small number of mid-sized files (somewhere between 1 MB and 100 MB) to minimize request pricing and overload.
#* A smaller number of larger files reduces the number of requests needed to retrieve and load the data, as well as to write the data. 
#* Since there is a small amount of latency associated with each file read, distributed computing processes (such as Hadoop-based or Apache Spark-based processes) that read files will generally proceed faster with a small number of mid-sized files than with a large number of small files.
#* The fewer your overall number of files, the less costly it is to run queries that try to match arbitrary regular expressions.
#* An important caveat is that, in many cases, the natural output type is a large number of small files. This is true for the outputs of distributed computing workloads, where each node in the cluster computes and outputs a small file. It is also true if data is being written out in real time and we want to write out the data within a short time interval. If you expect to read and process this data repeatedly, consider coalescing the data into larger files. Also, for data coming in in real time, consider using streaming services such as Kinesis to collate data before writing it out to S3.
#If using tools like Apache Spark, investigate whether the regexes used in Spark could have been creating problems. Also investigate whether a malfunctioning Spark process that goes in an infinite loop could be generating the high request count. You will generally see the effect of this in a large number of LIST and HEADOBJECT requests.

=== Monitoring and debugging ===
#Set up monitoring for your S3 costs.[[Image:Optimize Your Amazon S3 Costs Step 15.jpg|center]]
#* Your AWS account has access to the billing data that provides the full breakdown of costs.  Set up a billing alert so that the data starts getting sent to Amazon CloudWatch. You can then set up more alerts using CloudWatch.<ref>[https://aws.amazon.com/blogs/aws/monitor-estimated-costs-using-amazon-cloudwatch-billing-metrics-and-alarms/ Monitor Estimated Charges Using Billing Alerts]</ref> CloudWatch data comes in as data points every few hours, but does not include a detailed breakdown along all the dimensions of interest.
#* At any time, you can download detailed breakdown by hour and service type from your root account. This data is usually 24-48 hours late, i.e., you will not see information for the most recent 24-48 hours. For S3, you can download in a spreadsheet or CSV format the data with breakdown by hour, bucket, region, and operation type (GET, POST, LIST, PUT, DELETE, HEADOBJECT, or whatever your operations are).
#Write scripts to give easy-to-read daily reports of your costs broken down in various ways.[[Image:Optimize Your Amazon S3 Costs Step 16.jpg|center]]
#* At the high level, you may wish to report a breakdown of your costs between storage, transfer, and request pricing.
#* Within each of these, you may want to break costs down further based on the storage class (Standard, RRS, IA, and Glacier).
#* Within request pricing, you may want to break down costs by the operation type (GET, POST, LIST, PUT, DELETE, HEADOBJECT, or whatever your operations are).
#* You can also provide a breakdown by bucket.
#* As a general rule, you need to decide the number of dimensions you drill down by trading off ease of quick understanding against sufficient granularity. A generally good tradeoff is to include drilldowns along one dimension at a time (e.g., one drilldown by bucket, one drilldown by storage vs. transfer vs. request pricing, one drilldown by storage class) in your daily report, and drill down further only if something seems out of the ordinary.
#Build an expected cost model and use your script to identify discrepancies between actual costs and your model.
#* Without a model of what costs ''should'' be, it's hard to take a look at the costs and know if they're wrong.
#* The process of building a cost model is a good exercise in clearly articulating your architecture and potentially thinking of improvements even without looking at the pattern of actual costs.
#Debug high costs.[[Image:Optimize Your Amazon S3 Costs Step 18.jpg|center]]
#* If the culprit is storage costs, see Part 3.
#* If the culprit is huge data transfer costs, see Part 4.
#* If the culprit is request pricing, see Part 5.

== Tips ==
*Keep track of your Amazon S3 costs. You cannot optimize what you do not measure. One of the biggest advantages of S3 is that you don't have to think too hard about file storage: you have effectively unlimited file storage that's not tied to a particular instance. This offers a lot of flexibility, but on the other hand it also means you might lose track of how much data you are using and how much it is costing you. You should periodically review your S3 costs, and also set up AWS Billing Alerts in Amazon CloudWatch to alert you when S3 costs for a given month exceed a threshold.<ref>[https://aws.amazon.com/blogs/aws/monitor-estimated-costs-using-amazon-cloudwatch-billing-metrics-and-alarms/ Monitor Estimated Charges Using Billing Alerts]</ref>
*Do not use Amazon S3 for any operations that require latencies of under a second. You may use S3 to initiate instances that run such operations, or for periodic refreshes of the data and configurations on these instances, but don't rely on S3 file GETs for operations where you need to respond within milliseconds. If using S3 for logging, buffer the activities locally on your frontend instance (or write them to a stream such as Kinesis) and then log them periodically to S3.
*Do not use S3 for applications that involve frequent reading and writing of data. Amazon S3 is suited more for medium-term and long-term data logging than as a place to store and rapidly update and look up data. Consider using databases (and other data stores) for rapidly updating data. One important thing to remember with S3 is that immediate read/write consistency is not guaranteed: it may take a few seconds after a write for the read to fetch the newly written file. Moreover, you will also see a huge bill if you try to use it this way, because request pricing gets pretty high if you use S3 this way.

== Related wikiHows ==
*[[Use Amazon Route 53]]
*[[Upload to Amazon S3]]
*[[Optimize Your Amazon EC2 Costs]]
*[[Use Amazon EC2 Spot Instances]]

== Sources and Citations ==
{{reflist}}

__PARTS__
